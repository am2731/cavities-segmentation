{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:37:18.265692Z","iopub.status.busy":"2023-05-27T14:37:18.265062Z","iopub.status.idle":"2023-05-27T14:37:32.283816Z","shell.execute_reply":"2023-05-27T14:37:32.282359Z","shell.execute_reply.started":"2023-05-27T14:37:18.265656Z"},"trusted":true},"outputs":[],"source":["!pip install natsort"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:38:20.148771Z","iopub.status.busy":"2023-05-27T14:38:20.148033Z","iopub.status.idle":"2023-05-27T14:38:30.954645Z","shell.execute_reply":"2023-05-27T14:38:30.953252Z","shell.execute_reply.started":"2023-05-27T14:38:20.148731Z"},"trusted":true},"outputs":[],"source":["\n","import os\n","import cv2\n","import numpy as np\n","from PIL import Image,ImageOps\n","from zipfile import ZipFile\n","from natsort import natsorted\n","\n","\n","def crop_img(img, scale=1.0):\n","    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n","    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n","    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n","    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n","    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n","    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n","    im_pil = Image.fromarray(img_cropped)\n","    im_pil = ImageOps.grayscale(im_pil)\n","    return im_pil\n","\n","\n","\n","def convert_one_channel(image):\n","\n","    if len(image.shape)>2:\n","        image=image[:,:,0]\n","        return image\n","    else:\n","        return image\n","    \n","def images(resize_shape, path):\n","   \n","    dirs=natsorted(os.listdir(path))\n","    images=image=Image.open(path+dirs[0]).convert('RGB')\n","    images=(images.resize((resize_shape),Image.ANTIALIAS))\n","    images=convert_one_channel(np.asarray(images))\n","    for i in range (1,len(dirs)):\n","        image=Image.open(path+dirs[i]).convert('RGB')\n","        image=image.resize((resize_shape),Image.ANTIALIAS)\n","        image=convert_one_channel(np.asarray(image))\n","        images=np.concatenate((images,image))\n","        \n","    images=np.reshape(images,(len(dirs),resize_shape[0],resize_shape[1],1))\n","    \n","    return images\n","\n","path=\"/kaggle/input/croppedcavities/croppedX/\"\n","\n","X=images((512,512),path)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:39:18.419896Z","iopub.status.busy":"2023-05-27T14:39:18.419454Z","iopub.status.idle":"2023-05-27T14:39:26.652868Z","shell.execute_reply":"2023-05-27T14:39:26.651756Z","shell.execute_reply.started":"2023-05-27T14:39:18.419858Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import numpy as np\n","import cv2\n","from PIL import Image\n","from zipfile import ZipFile\n","from natsort import natsorted\n","\n","def convert_one_channel(img):\n","    if len(img.shape)>2:\n","        img=img[:,:,0]\n","        return img\n","    else:\n","        return img\n","\n","\n","def masks(path):\n","    path=\"/kaggle/input/croppedcavities/croppedYCavities/\"\n","    dirs=natsorted(os.listdir(path))\n","    masks=image=Image.open(path+dirs[0])\n","    masks=convert_one_channel(np.asarray(masks))\n","    for i in range (1,len(dirs)):\n","        image=Image.open(path+dirs[i])\n","        image=convert_one_channel(np.asarray(image))\n","        masks=np.concatenate((masks,image))\n","    masks=np.reshape(masks,(len(dirs),512,512,1))\n","    return masks\n","    \n","\n","Y=masks(path=\"/kaggle/input/croppedcavities/croppedYCavities/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:42:56.098420Z","iopub.status.busy":"2023-05-27T14:42:56.097229Z","iopub.status.idle":"2023-05-27T14:43:21.728305Z","shell.execute_reply":"2023-05-27T14:43:21.727016Z","shell.execute_reply.started":"2023-05-27T14:42:56.098371Z"},"trusted":true},"outputs":[],"source":["import math\n","src=X[0,:,:,0]\n","mid = 0.5\n","mean = np.mean(src)\n","gamma = math.log(mid*255)/math.log(mean)\n","        \n","X2= np.power(src, gamma).clip(0,255).astype(np.uint8)\n","X2 = cv2.cvtColor(X2,cv2.COLOR_GRAY2RGB)\n","\n","for i in range(len(X)):\n","    if i!=0:\n","        src=X[i,:,:,0]\n","        mid = 0.5\n","        mean = np.mean(src)\n","        gamma = math.log(mid*255)/math.log(mean)\n","        dst = np.power(src, gamma).clip(0,255).astype(np.uint8)\n","        dst = cv2.cvtColor(dst,cv2.COLOR_GRAY2RGB) #for 3 channels\n","        X2=np.concatenate((X2,dst))\n","X2=np.reshape(X2,(390,512,512,3))\n","print(X2.shape)\n","X=X2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X=np.float32(X/255)\n","Y=np.float32(Y/255)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:44:17.566703Z","iopub.status.busy":"2023-05-27T14:44:17.566078Z","iopub.status.idle":"2023-05-27T14:44:17.585201Z","shell.execute_reply":"2023-05-27T14:44:17.583824Z","shell.execute_reply.started":"2023-05-27T14:44:17.566646Z"},"trusted":true},"outputs":[],"source":["x_train=X[:97,:,:,:]\n","y_train=Y[:97,:,:,:]\n","x_test=X[97:,:,:,:]\n","y_test=Y[97:,:,:,:]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.imshow(Y[100,:,:,0], cmap=\"gray\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.imshow(X[17,:,:,0], cmap=\"gray\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T12:14:07.491368Z","iopub.status.busy":"2023-05-21T12:14:07.490972Z","iopub.status.idle":"2023-05-21T12:14:13.724442Z","shell.execute_reply":"2023-05-21T12:14:13.723258Z","shell.execute_reply.started":"2023-05-21T12:14:07.491328Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["376\n"]}],"source":["import cv2\n","\n","import albumentations as A\n","\n","\n","aug = A.Compose([\n","    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.35,p=0.5),\n","    A.ShiftScaleRotate (shift_limit=0.10, scale_limit=0.10, rotate_limit=15,border_mode=cv2.BORDER_CONSTANT, p=1),\n","    A.Rotate(15,p=0.5),\n","    A.HorizontalFlip(p=0.45),\n","    A.Sharpen(p=0.2),\n","])\n","\n","\n","x_train1=np.copy(x_train)\n","y_train1=np.copy(y_train)\n","count=0\n","while(count<2):\n","  x_aug2=np.copy(x_train1)\n","  y_aug2=np.copy(y_train1)\n","  for i in range(len(x_train1)):\n","    augmented=aug(image=x_train1[i,:,:,:],mask=y_train1[i,:,:,:])\n","    x_aug2[i,:,:,:]= augmented['image']\n","    y_aug2[i,:,:,:]= augmented['mask']\n","  x_train=np.concatenate((x_train,x_aug2))\n","  y_train=np.concatenate((y_train,y_aug2))\n","  if count == 9:\n","    break\n","  count += 1\n","x_train=np.concatenate((x_train,x_train1))\n","y_train=np.concatenate((y_train,y_train1))\n","print(len(x_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:45:17.430038Z","iopub.status.busy":"2023-05-27T14:45:17.428826Z","iopub.status.idle":"2023-05-27T14:45:17.437628Z","shell.execute_reply":"2023-05-27T14:45:17.436477Z","shell.execute_reply.started":"2023-05-27T14:45:17.429980Z"},"trusted":true},"outputs":[],"source":["%env SM_FRAMEWORK=tf.keras"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:44:58.737469Z","iopub.status.busy":"2023-05-27T14:44:58.736592Z","iopub.status.idle":"2023-05-27T14:45:15.239949Z","shell.execute_reply":"2023-05-27T14:45:15.238602Z","shell.execute_reply.started":"2023-05-27T14:44:58.737426Z"},"trusted":true},"outputs":[],"source":["!pip install git+https://github.com/qubvel/segmentation_models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:45:19.907686Z","iopub.status.busy":"2023-05-27T14:45:19.906941Z","iopub.status.idle":"2023-05-27T14:45:27.882844Z","shell.execute_reply":"2023-05-27T14:45:27.880511Z","shell.execute_reply.started":"2023-05-27T14:45:19.907641Z"},"trusted":true},"outputs":[],"source":["import segmentation_models as sm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T12:15:14.951316Z","iopub.status.busy":"2023-05-21T12:15:14.950469Z","iopub.status.idle":"2023-05-21T12:15:15.044796Z","shell.execute_reply":"2023-05-21T12:15:15.043642Z","shell.execute_reply.started":"2023-05-21T12:15:14.951262Z"},"trusted":true},"outputs":[],"source":["#source: bonlime (Github)\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.layers import Add\n","from tensorflow.keras.layers import Dropout\n","from keras.layers.normalization.batch_normalization import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import DepthwiseConv2D\n","from tensorflow.keras.layers import ZeroPadding2D\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.python.keras.utils.layer_utils import get_source_inputs\n","from tensorflow.python.keras.utils.data_utils import get_file\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.applications.imagenet_utils import preprocess_input\n","\n","WEIGHTS_PATH_X = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\"\n","WEIGHTS_PATH_MOBILE = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5\"\n","WEIGHTS_PATH_X_CS = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.2/deeplabv3_xception_tf_dim_ordering_tf_kernels_cityscapes.h5\"\n","WEIGHTS_PATH_MOBILE_CS = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.2/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels_cityscapes.h5\"\n","\n","\n","def SepConv_BN(x, filters, prefix, stride=1, kernel_size=3, rate=1, depth_activation=False, epsilon=1e-3):\n","    \"\"\" SepConv with BN between depthwise & pointwise. Optionally add activation after BN\n","        Implements right \"same\" padding for even kernel sizes\n","        Args:\n","            x: input tensor\n","            filters: num of filters in pointwise convolution\n","            prefix: prefix before name\n","            stride: stride at depthwise conv\n","            kernel_size: kernel size for depthwise convolution\n","            rate: atrous rate for depthwise convolution\n","            depth_activation: flag to use activation between depthwise & poinwise convs\n","            epsilon: epsilon to use in BN layer\n","    \"\"\"\n","\n","    if stride == 1:\n","        depth_padding = 'same'\n","    else:\n","        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n","        pad_total = kernel_size_effective - 1\n","        pad_beg = pad_total // 2\n","        pad_end = pad_total - pad_beg\n","        x = ZeroPadding2D((pad_beg, pad_end))(x)\n","        depth_padding = 'valid'\n","\n","    if not depth_activation:\n","        x = Activation(tf.nn.relu)(x)\n","    x = DepthwiseConv2D((kernel_size, kernel_size), strides=(stride, stride), dilation_rate=(rate, rate),\n","                        padding=depth_padding, use_bias=False, name=prefix + '_depthwise')(x)\n","    x = BatchNormalization(name=prefix + '_depthwise_BN', epsilon=epsilon)(x)\n","    if depth_activation:\n","        x = Activation(tf.nn.relu)(x)\n","    x = Conv2D(filters, (1, 1), padding='same',\n","               use_bias=False, name=prefix + '_pointwise')(x)\n","    x = BatchNormalization(name=prefix + '_pointwise_BN', epsilon=epsilon)(x)\n","    if depth_activation:\n","        x = Activation(tf.nn.relu)(x)\n","\n","    return x\n","\n","\n","def _conv2d_same(x, filters, prefix, stride=1, kernel_size=3, rate=1):\n","    \"\"\"Implements right 'same' padding for even kernel sizes\n","        Without this there is a 1 pixel drift when stride = 2\n","        Args:\n","            x: input tensor\n","            filters: num of filters in pointwise convolution\n","            prefix: prefix before name\n","            stride: stride at depthwise conv\n","            kernel_size: kernel size for depthwise convolution\n","            rate: atrous rate for depthwise convolution\n","    \"\"\"\n","    if stride == 1:\n","        return Conv2D(filters,\n","                      (kernel_size, kernel_size),\n","                      strides=(stride, stride),\n","                      padding='same', use_bias=False,\n","                      dilation_rate=(rate, rate),\n","                      name=prefix)(x)\n","    else:\n","        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n","        pad_total = kernel_size_effective - 1\n","        pad_beg = pad_total // 2\n","        pad_end = pad_total - pad_beg\n","        x = ZeroPadding2D((pad_beg, pad_end))(x)\n","        return Conv2D(filters,\n","                      (kernel_size, kernel_size),\n","                      strides=(stride, stride),\n","                      padding='valid', use_bias=False,\n","                      dilation_rate=(rate, rate),\n","                      name=prefix)(x)\n","\n","\n","def _xception_block(inputs, depth_list, prefix, skip_connection_type, stride,\n","                    rate=1, depth_activation=False, return_skip=False):\n","    \"\"\" Basic building block of modified Xception network\n","        Args:\n","            inputs: input tensor\n","            depth_list: number of filters in each SepConv layer. len(depth_list) == 3\n","            prefix: prefix before name\n","            skip_connection_type: one of {'conv','sum','none'}\n","            stride: stride at last depthwise conv\n","            rate: atrous rate for depthwise convolution\n","            depth_activation: flag to use activation between depthwise & pointwise convs\n","            return_skip: flag to return additional tensor after 2 SepConvs for decoder\n","            \"\"\"\n","    residual = inputs\n","    for i in range(3):\n","        residual = SepConv_BN(residual,\n","                              depth_list[i],\n","                              prefix + '_separable_conv{}'.format(i + 1),\n","                              stride=stride if i == 2 else 1,\n","                              rate=rate,\n","                              depth_activation=depth_activation)\n","        if i == 1:\n","            skip = residual\n","    if skip_connection_type == 'conv':\n","        shortcut = _conv2d_same(inputs, depth_list[-1], prefix + '_shortcut',\n","                                kernel_size=1,\n","                                stride=stride)\n","        shortcut = BatchNormalization(name=prefix + '_shortcut_BN')(shortcut)\n","        outputs = layers.add([residual, shortcut])\n","    elif skip_connection_type == 'sum':\n","        outputs = layers.add([residual, inputs])\n","    elif skip_connection_type == 'none':\n","        outputs = residual\n","    if return_skip:\n","        return outputs, skip\n","    else:\n","        return outputs\n","\n","\n","def _make_divisible(v, divisor, min_value=None):\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id, skip_connection, rate=1):\n","    in_channels = inputs.shape[-1]\n","    pointwise_conv_filters = int(filters * alpha)\n","    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n","    x = inputs\n","    prefix = 'expanded_conv_{}_'.format(block_id)\n","    if block_id:\n","        # Expand\n","\n","        x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',\n","                   use_bias=False, activation=None,\n","                   name=prefix + 'expand')(x)\n","        x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n","                               name=prefix + 'expand_BN')(x)\n","        x = Activation(tf.nn.relu6, name=prefix + 'expand_relu')(x)\n","    else:\n","        prefix = 'expanded_conv_'\n","    # Depthwise\n","    x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,\n","                        use_bias=False, padding='same', dilation_rate=(rate, rate),\n","                        name=prefix + 'depthwise')(x)\n","    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n","                           name=prefix + 'depthwise_BN')(x)\n","\n","    x = Activation(tf.nn.relu6, name=prefix + 'depthwise_relu')(x)\n","\n","    # Project\n","    x = Conv2D(pointwise_filters,\n","               kernel_size=1, padding='same', use_bias=False, activation=None,\n","               name=prefix + 'project')(x)\n","    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n","                           name=prefix + 'project_BN')(x)\n","\n","    if skip_connection:\n","        return Add(name=prefix + 'add')([inputs, x])\n","\n","    # if in_channels == pointwise_filters and stride == 1:\n","    #    return Add(name='res_connect_' + str(block_id))([inputs, x])\n","\n","    return x\n","\n","\n","def Deeplabv3(weights='pascal_voc', input_tensor=None, input_shape=(512, 512, 3), classes=21, backbone='mobilenetv2',\n","              OS=16, alpha=1., activation=None):\n","    \"\"\" Instantiates the Deeplabv3+ architecture\n","    Optionally loads weights pre-trained\n","    on PASCAL VOC or Cityscapes. This model is available for TensorFlow only.\n","    # Arguments\n","        weights: one of 'pascal_voc' (pre-trained on pascal voc),\n","            'cityscapes' (pre-trained on cityscape) or None (random initialization)\n","        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n","            to use as image input for the model.\n","        input_shape: shape of input image. format HxWxC\n","            PASCAL VOC model was trained on (512,512,3) images. None is allowed as shape/width\n","        classes: number of desired classes. PASCAL VOC has 21 classes, Cityscapes has 19 classes.\n","            If number of classes not aligned with the weights used, last layer is initialized randomly\n","        backbone: backbone to use. one of {'xception','mobilenetv2'}\n","        activation: optional activation to add to the top of the network.\n","            One of 'softmax', 'sigmoid' or None\n","        OS: determines input_shape/feature_extractor_output ratio. One of {8,16}.\n","            Used only for xception backbone.\n","        alpha: controls the width of the MobileNetV2 network. This is known as the\n","            width multiplier in the MobileNetV2 paper.\n","                - If `alpha` < 1.0, proportionally decreases the number\n","                    of filters in each layer.\n","                - If `alpha` > 1.0, proportionally increases the number\n","                    of filters in each layer.\n","                - If `alpha` = 1, default number of filters from the paper\n","                    are used at each layer.\n","            Used only for mobilenetv2 backbone. Pretrained is only available for alpha=1.\n","    # Returns\n","        A Keras model instance.\n","    # Raises\n","        RuntimeError: If attempting to run this model with a\n","            backend that does not support separable convolutions.\n","        ValueError: in case of invalid argument for `weights` or `backbone`\n","    \"\"\"\n","\n","    if not (weights in {'pascal_voc', 'cityscapes', None}):\n","        raise ValueError('The `weights` argument should be either '\n","                         '`None` (random initialization), `pascal_voc`, or `cityscapes` '\n","                         '(pre-trained on PASCAL VOC)')\n","\n","    if not (backbone in {'xception', 'mobilenetv2'}):\n","        raise ValueError('The `backbone` argument should be either '\n","                         '`xception`  or `mobilenetv2` ')\n","\n","    if input_tensor is None:\n","        img_input = Input(shape=input_shape)\n","    else:\n","        img_input = input_tensor\n","\n","    if backbone == 'xception':\n","        if OS == 8:\n","            entry_block3_stride = 1\n","            middle_block_rate = 2  # ! Not mentioned in paper, but required\n","            exit_block_rates = (2, 4)\n","            atrous_rates = (12, 24, 36)\n","        else:\n","            entry_block3_stride = 2\n","            middle_block_rate = 1\n","            exit_block_rates = (1, 2)\n","            atrous_rates = (6, 12, 18)\n","\n","        x = Conv2D(32, (3, 3), strides=(2, 2),\n","                   name='entry_flow_conv1_1', use_bias=False, padding='same')(img_input)\n","        x = BatchNormalization(name='entry_flow_conv1_1_BN')(x)\n","        x = Activation(tf.nn.relu)(x)\n","\n","        x = _conv2d_same(x, 64, 'entry_flow_conv1_2', kernel_size=3, stride=1)\n","        x = BatchNormalization(name='entry_flow_conv1_2_BN')(x)\n","        x = Activation(tf.nn.relu)(x)\n","\n","        x = _xception_block(x, [128, 128, 128], 'entry_flow_block1',\n","                            skip_connection_type='conv', stride=2,\n","                            depth_activation=False)\n","        x, skip1 = _xception_block(x, [256, 256, 256], 'entry_flow_block2',\n","                                   skip_connection_type='conv', stride=2,\n","                                   depth_activation=False, return_skip=True)\n","\n","        x = _xception_block(x, [728, 728, 728], 'entry_flow_block3',\n","                            skip_connection_type='conv', stride=entry_block3_stride,\n","                            depth_activation=False)\n","        for i in range(16):\n","            x = _xception_block(x, [728, 728, 728], 'middle_flow_unit_{}'.format(i + 1),\n","                                skip_connection_type='sum', stride=1, rate=middle_block_rate,\n","                                depth_activation=False)\n","\n","        x = _xception_block(x, [728, 1024, 1024], 'exit_flow_block1',\n","                            skip_connection_type='conv', stride=1, rate=exit_block_rates[0],\n","                            depth_activation=False)\n","        x = _xception_block(x, [1536, 1536, 2048], 'exit_flow_block2',\n","                            skip_connection_type='none', stride=1, rate=exit_block_rates[1],\n","                            depth_activation=True)\n","\n","    else:\n","        OS = 8\n","        first_block_filters = _make_divisible(32 * alpha, 8)\n","        x = Conv2D(first_block_filters,\n","                   kernel_size=3,\n","                   strides=(2, 2), padding='same', use_bias=False,\n","                   name='Conv' if input_shape[2] == 3 else 'Conv_')(img_input)\n","        x = BatchNormalization(\n","            epsilon=1e-3, momentum=0.999, name='Conv_BN')(x)\n","        x = Activation(tf.nn.relu6, name='Conv_Relu6')(x)\n","\n","        x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n","                                expansion=1, block_id=0, skip_connection=False)\n","\n","        x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n","                                expansion=6, block_id=1, skip_connection=False)\n","        x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n","                                expansion=6, block_id=2, skip_connection=True)\n","\n","        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n","                                expansion=6, block_id=3, skip_connection=False)\n","        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n","                                expansion=6, block_id=4, skip_connection=True)\n","        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n","                                expansion=6, block_id=5, skip_connection=True)\n","\n","        # stride in block 6 changed from 2 -> 1, so we need to use rate = 2\n","        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,  # 1!\n","                                expansion=6, block_id=6, skip_connection=False)\n","        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n","                                expansion=6, block_id=7, skip_connection=True)\n","        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n","                                expansion=6, block_id=8, skip_connection=True)\n","        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n","                                expansion=6, block_id=9, skip_connection=True)\n","\n","        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n","                                expansion=6, block_id=10, skip_connection=False)\n","        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n","                                expansion=6, block_id=11, skip_connection=True)\n","        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n","                                expansion=6, block_id=12, skip_connection=True)\n","\n","        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=2,  # 1!\n","                                expansion=6, block_id=13, skip_connection=False)\n","        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=4,\n","                                expansion=6, block_id=14, skip_connection=True)\n","        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=4,\n","                                expansion=6, block_id=15, skip_connection=True)\n","\n","        x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1, rate=4,\n","                                expansion=6, block_id=16, skip_connection=False)\n","\n","    # end of feature extractor\n","\n","    # branching for Atrous Spatial Pyramid Pooling\n","\n","    # Image Feature branch\n","    shape_before = tf.shape(x)\n","    b4 = GlobalAveragePooling2D()(x)\n","    b4_shape = tf.keras.backend.int_shape(b4)\n","    # from (b_size, channels)->(b_size, 1, 1, channels)\n","    b4 = Reshape((1, 1, b4_shape[1]))(b4)\n","    b4 = Conv2D(256, (1, 1), padding='same',\n","                use_bias=False, name='image_pooling')(b4)\n","    b4 = BatchNormalization(name='image_pooling_BN', epsilon=1e-5)(b4)\n","    b4 = Activation(tf.nn.relu)(b4)\n","    # upsample. have to use compat because of the option align_corners\n","    size_before = tf.keras.backend.int_shape(x)\n","    b4 = tf.keras.layers.experimental.preprocessing.Resizing(\n","            *size_before[1:3], interpolation=\"bilinear\"\n","        )(b4)\n","    # simple 1x1\n","    b0 = Conv2D(256, (1, 1), padding='same', use_bias=False, name='aspp0')(x)\n","    b0 = BatchNormalization(name='aspp0_BN', epsilon=1e-5)(b0)\n","    b0 = Activation(tf.nn.relu, name='aspp0_activation')(b0)\n","\n","    # there are only 2 branches in mobilenetV2. not sure why\n","    if backbone == 'xception':\n","        # rate = 6 (12)\n","        b1 = SepConv_BN(x, 256, 'aspp1',\n","                        rate=atrous_rates[0], depth_activation=True, epsilon=1e-5)\n","        # rate = 12 (24)\n","        b2 = SepConv_BN(x, 256, 'aspp2',\n","                        rate=atrous_rates[1], depth_activation=True, epsilon=1e-5)\n","        # rate = 18 (36)\n","        b3 = SepConv_BN(x, 256, 'aspp3',\n","                        rate=atrous_rates[2], depth_activation=True, epsilon=1e-5)\n","\n","        # concatenate ASPP branches & project\n","        x = Concatenate()([b4, b0, b1, b2, b3])\n","    else:\n","        x = Concatenate()([b4, b0])\n","\n","    x = Conv2D(256, (1, 1), padding='same',\n","               use_bias=False, name='concat_projection')(x)\n","    x = BatchNormalization(name='concat_projection_BN', epsilon=1e-5)(x)\n","    x = Activation(tf.nn.relu)(x)\n","    x = Dropout(0.1)(x)\n","    # DeepLab v.3+ decoder\n","\n","    if backbone == 'xception':\n","        # Feature projection\n","        # x4 (x2) block\n","        skip_size = tf.keras.backend.int_shape(skip1)\n","        x = tf.keras.layers.experimental.preprocessing.Resizing(\n","                *skip_size[1:3], interpolation=\"bilinear\"\n","            )(x)\n","        dec_skip1 = Conv2D(48, (1, 1), padding='same',\n","                           use_bias=False, name='feature_projection0')(skip1)\n","        dec_skip1 = BatchNormalization(\n","            name='feature_projection0_BN', epsilon=1e-5)(dec_skip1)\n","        dec_skip1 = Activation(tf.nn.relu)(dec_skip1)\n","        x = Concatenate()([x, dec_skip1])\n","        x = SepConv_BN(x, 256, 'decoder_conv0',\n","                       depth_activation=True, epsilon=1e-5)\n","        x = SepConv_BN(x, 256, 'decoder_conv1',\n","                       depth_activation=True, epsilon=1e-5)\n","\n","    # you can use it with arbitary number of classes\n","    if (weights == 'pascal_voc' and classes == 21) or (weights == 'cityscapes' and classes == 19):\n","        last_layer_name = 'logits_semantic'\n","    else:\n","        last_layer_name = 'custom_logits_semantic'\n","\n","    x = Conv2D(classes, (1, 1), padding='same', name=last_layer_name)(x)\n","    size_before3 = tf.keras.backend.int_shape(img_input)\n","    x = tf.keras.layers.experimental.preprocessing.Resizing(\n","            *size_before3[1:3], interpolation=\"bilinear\"\n","        )(x)\n","    # Ensure that the model takes into account\n","    # any potential predecessors of `input_tensor`.\n","    if input_tensor is not None:\n","        inputs = get_source_inputs(input_tensor)\n","    else:\n","        inputs = img_input\n","\n","    if activation in {'softmax', 'sigmoid'}:\n","        x = tf.keras.layers.Activation(activation)(x)\n","\n","    model = Model(inputs, x, name='deeplabv3plus')\n","\n","    # load weights\n","\n","    if weights == 'pascal_voc':\n","        if backbone == 'xception':\n","            weights_path = get_file('deeplabv3_xception_tf_dim_ordering_tf_kernels.h5',\n","                                    WEIGHTS_PATH_X,\n","                                    cache_subdir='models')\n","        else:\n","            weights_path = get_file('deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5',\n","                                    WEIGHTS_PATH_MOBILE,\n","                                    cache_subdir='models')\n","        model.load_weights(weights_path, by_name=True)\n","    elif weights == 'cityscapes':\n","        if backbone == 'xception':\n","            weights_path = get_file('deeplabv3_xception_tf_dim_ordering_tf_kernels_cityscapes.h5',\n","                                    WEIGHTS_PATH_X_CS,\n","                                    cache_subdir='models')\n","        else:\n","            weights_path = get_file('deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels_cityscapes.h5',\n","                                    WEIGHTS_PATH_MOBILE_CS,\n","                                    cache_subdir='models')\n","        model.load_weights(weights_path, by_name=True)\n","    return model\n","\n","def preprocess_input(x):\n","    \"\"\"Preprocesses a numpy array encoding a batch of images.\n","    # Arguments\n","        x: a 4D numpy array consists of RGB values within [0, 255].\n","    # Returns\n","        Input array scaled to [-1.,1.]\n","    \"\"\"\n","    return preprocess_input(x, mode='tf')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T12:15:32.587734Z","iopub.status.busy":"2023-05-21T12:15:32.587180Z","iopub.status.idle":"2023-05-21T14:24:45.355357Z","shell.execute_reply":"2023-05-21T14:24:45.354312Z","shell.execute_reply.started":"2023-05-21T12:15:32.587696Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping\n","from keras.losses import binary_crossentropy\n","def tversky_coef(y_true, y_pred, alpha=0.7, const=K.epsilon()):\n","    '''\n","    Weighted Sørensen–Dice coefficient.\n","    \n","    Input\n","    ----------\n","        y_true, y_pred: predicted outputs and targets.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    \n","    # flatten 2-d tensors\n","    y_true_pos = tf.reshape(y_true, [-1])\n","    y_pred_pos = tf.reshape(y_pred, [-1])\n","    \n","    # get true pos (TP), false neg (FN), false pos (FP).\n","    true_pos  = tf.reduce_sum(y_true_pos * y_pred_pos)\n","    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n","    false_pos = tf.reduce_sum((1-y_true_pos) * y_pred_pos)\n","    \n","    # TP/(TP + a*FN + b*FP); a+b = 1\n","    coef_val = (true_pos + const)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + const)\n","    \n","    return coef_val\n","\n","def tversky(y_true, y_pred, alpha=0.7, const=K.epsilon()):\n","    '''\n","    Tversky Loss.\n","    \n","    tversky(y_true, y_pred, alpha=0.5, const=K.epsilon())\n","    \n","    ----------\n","    Hashemi, S.R., Salehi, S.S.M., Erdogmus, D., Prabhu, S.P., Warfield, S.K. and Gholipour, A., 2018. \n","    Tversky as a loss function for highly unbalanced image segmentation using 3d fully convolutional deep networks. \n","    arXiv preprint arXiv:1803.11078.\n","    \n","    Input\n","    ----------\n","        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    loss_val = 1 - tversky_coef(y_true, y_pred, alpha=alpha, const=const)\n","    \n","    return loss_val\n","\n","def focal_tversky(y_true, y_pred, alpha=0.7, gamma=4/3, const=K.epsilon()):\n","    \n","    '''\n","    Focal Tversky Loss (FTL)\n","    \n","    focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n","    \n","    ----------\n","    Abraham, N. and Khan, N.M., 2019, April. A novel focal tversky loss function with improved \n","    attention u-net for lesion segmentation. In 2019 IEEE 16th International Symposium on Biomedical Imaging \n","    (ISBI 2019) (pp. 683-687). IEEE.\n","    \n","    ----------\n","    Input\n","        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases \n","        gamma: tunable parameter within [1, 3].\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    # (Tversky loss)**(1/gamma) \n","    loss_val = tf.math.pow((1-tversky_coef(y_true, y_pred, alpha=alpha, const=const)), 1/gamma)\n","    \n","    return loss_val\n","\n","\n","def dice_coef(y_true, y_pred, const=K.epsilon()):\n","    '''\n","    Sørensen–Dice coefficient for 2-d samples.\n","    \n","    Input\n","    ----------\n","        y_true, y_pred: predicted outputs and targets.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    \n","    # flatten 2-d tensors\n","    y_true_pos = tf.reshape(y_true, [-1])\n","    y_pred_pos = tf.reshape(y_pred, [-1])\n","    \n","    # get true pos (TP), false neg (FN), false pos (FP).\n","    true_pos  = tf.reduce_sum(y_true_pos * y_pred_pos)\n","    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n","    false_pos = tf.reduce_sum((1-y_true_pos) * y_pred_pos)\n","    \n","    # 2TP/(2TP+FP+FN) == 2TP/()\n","    coef_val = (2.0 * true_pos + const)/(2.0 * true_pos + false_pos + false_neg)\n","    \n","    return coef_val\n","\n","def dice(y_true, y_pred, const=K.epsilon()):\n","    '''\n","    Sørensen–Dice Loss.\n","    \n","    dice(y_true, y_pred, const=K.epsilon())\n","    \n","    Input\n","    ----------\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    loss_val = 1 - dice_coef(y_true, y_pred, const=const)\n","    \n","    #BCE =  K.binary_crossentropy(y_true,y_pred)\n","    return loss_val # + BCE\n","\n","def dsc(y_true, y_pred):\n","    smooth = 1.\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    return score\n","\n","def dice_loss(y_true, y_pred):\n","    loss = 1 - dsc(y_true, y_pred)\n","    return loss\n","\n","def bce_dice_loss(y_true, y_pred):\n","    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","    return loss\n","\n","def DiceBCELoss2(targets, inputs, smooth=1e-6):    \n","       \n","    #flatten label and prediction tensors\n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","    \n","    BCE =  K.binary_crossentropy(targets, inputs)\n","    intersection = K.sum(inputs * targets)\n","    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    Dice_BCE = BCE + dice_loss\n","    \n","    return Dice_BCE\n","\n","\n","ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\n","CE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n","smooth=1\n","def Combo_loss2(targets, inputs, eps=K.epsilon()):\n","    targets = K.flatten(targets)\n","    inputs = K.flatten(inputs)\n","    \n","    intersection = K.sum(targets * inputs)\n","    dice = (2. * intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    inputs = K.clip(inputs, eps, 1.0 - eps)\n","    out = - (ALPHA * ((targets * K.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * K.log(1.0 - inputs))))\n","    weighted_ce = K.mean(out, axis=-1)\n","    combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n","    \n","    return combo\n","\n","opt = Adam(0.001)\n","\n","model = Deeplabv3(input_shape=(512, 512, 3), classes=1, backbone='xception', activation='sigmoid')\n","\n","model.compile(optimizer=opt, loss = focal_tversky, metrics=['accuracy', sm.metrics.IOUScore(threshold=0.5)]) \n","\n","history = model.fit(x_train,y_train,batch_size=4,epochs=100,verbose=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:31:32.662912Z","iopub.status.busy":"2023-05-27T16:31:32.661784Z","iopub.status.idle":"2023-05-27T17:10:07.106958Z","shell.execute_reply":"2023-05-27T17:10:07.105793Z","shell.execute_reply.started":"2023-05-27T16:31:32.662855Z"},"trusted":true},"outputs":[],"source":["#from keras_unet_collection.losses import  dice, iou_seg, focal_tversky, crps2d_tf\n","\n","import tensorflow as tf \n","\n","from keras.losses import LossFunctionWrapper\n","from keras import backend as K\n","from keras.utils import losses_utils\n","from keras.utils import tf_utils\n","from tensorflow.python.ops import array_ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.metrics import f1_score,accuracy_score\n","\n","from keras.optimizers import SGD\n","\n","\n","\n","\n","\n","def pixelwise_weighted_binary_crossentropy(y_true, y_pred):\n","    '''\n","    Pixel-wise weighted binary cross-entropy loss.\n","    The code is adapted from the Keras TF backend.\n","    (see their github)\n","    \n","    Parameters\n","    ----------\n","    y_true : Tensor\n","        Stack of groundtruth segmentation masks + weight maps.\n","    y_pred : Tensor\n","        Predicted segmentation masks.\n","\n","    Returns\n","    -------\n","    Tensor\n","        Pixel-wise weight binary cross-entropy between inputs.\n","\n","    '''\n","    \n","   \n","    [seg, weight] = tf.unstack(y_true, 2, axis=-1)\n","\n","    seg = tf.expand_dims(seg, -1)\n","    weight = tf.expand_dims(weight, -1)\n","    \n","\n","    epsilon = tf.convert_to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n","    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n","    y_pred = tf.math.log(y_pred / (1 - y_pred))\n","\n","    zeros = array_ops.zeros_like(y_pred, dtype=y_pred.dtype)\n","    cond = (y_pred >= zeros)\n","    relu_logits = math_ops.select(cond, y_pred, zeros)\n","    neg_abs_logits = math_ops.select(cond, -y_pred, y_pred)\n","    entropy = math_ops.add(relu_logits - y_pred * seg, math_ops.log1p(math_ops.exp(neg_abs_logits)), name=None)\n","    \n","    # This is essentially the only part that is different from the Keras code:\n","    return K.mean(math_ops.multiply(weight, entropy), axis=-1)\n","    \n","\n","def soft_dice_loss(y_true, y_pred, epsilon=1e-6):  # skip the batch and class axis for calculating Dice score a\n","    \n","    \"\"\"Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n","    Assumes the `channels_last` format.\n","  \n","    # Arguments\n","        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n","        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n","        epsilon: Used for numerical stability to avoid divide by zero errors\n","        # skip the batch and class axis for calculating Dice score\n","    axes = tuple(range(1, len(y_pred.shape)-1)) \n","    numerator = 2. * np.sum(y_pred * y_true, axes)\n","    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n","    \n","    return 1 - np.mean(numerator / (denominator + epsilon)) # average over classes and batch\n","    \"\"\"\n","    axes = tuple(range(1, len(y_pred.shape)-1)) \n","    numerator = 2. * tf.reduce_sum(y_pred * y_true, axes) \n","    denominator = tf.reduce_sum(tf.square(y_pred) + tf.square(y_true), axes) \n","    result = 1 - tf.reduce_mean((numerator + epsilon) / (denominator + epsilon)) \n","    return result # average over classes and batch\n","\n","#Accuracy :  94.78267669677734 teeth binary\n","\n","ce_w = 0.15\n","ce_d_w = 0.5\n","e = K.epsilon()\n","smooth = 1\n","'''\n","ce_w values smaller than 0.5 penalize false positives more while values larger than 0.5 penalize false negatives more\n","ce_d_w is level of contribution of the cross-entropy loss in the total loss.\n","'''\n","\n","def Combo_loss(y_true, y_pred):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    d = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    y_pred_f = K.clip(y_pred_f, e, 1.0 - e)\n","    out = - (ce_w * y_true_f * K.log(y_pred_f)) + ((1 - ce_w) * (1.0 - y_true_f) * K.log(1.0 - y_pred_f))\n","    weighted_ce = K.mean(out, axis=-1)\n","    combo = (ce_d_w * weighted_ce) - ((1 - ce_d_w) * d)\n","    return combo\n","\n","ALPHA = 0.8\n","GAMMA = 2\n","\n","def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n","    \n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","    \n","    BCE = K.binary_crossentropy(targets, inputs)\n","    BCE_EXP = K.exp(-BCE)\n","    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n","    \n","    return focal_loss\n","\n","#Keras\n","def DiceBCELoss(targets, inputs, smooth=1e-6):    \n","       \n","    #flatten label and prediction tensors\n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","    \n","    BCE =  K.binary_crossentropy(targets, inputs)\n","    intersection = K.sum(inputs * targets)\n","    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    Dice_BCE = 0.8*BCE + 0.5*dice_loss\n","    \n","    return Dice_BCE\n","\n","\n","def tversky2(y_true, y_pred):\n","    smooth=1\n","    y_true_pos = K.flatten(y_true)\n","    y_pred_pos = K.flatten(y_pred)\n","    true_pos = K.sum(y_true_pos * y_pred_pos)\n","    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n","    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n","    alpha = 0.7\n","    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n","\n","def tversky_loss(y_true, y_pred):\n","    return 1 - tversky(y_true,y_pred)\n","    \n","\n","def dice_coef(y_true, y_pred, const=K.epsilon()):\n","    '''\n","    Sørensen–Dice coefficient for 2-d samples.\n","    \n","    Input\n","    ----------\n","        y_true, y_pred: predicted outputs and targets.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    \n","    # flatten 2-d tensors\n","    y_true_pos = tf.reshape(y_true, [-1])\n","    y_pred_pos = tf.reshape(y_pred, [-1])\n","    \n","    # get true pos (TP), false neg (FN), false pos (FP).\n","    true_pos  = tf.reduce_sum(y_true_pos * y_pred_pos)\n","    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n","    false_pos = tf.reduce_sum((1-y_true_pos) * y_pred_pos)\n","    \n","    # 2TP/(2TP+FP+FN) == 2TP/()\n","    coef_val = (2.0 * true_pos + const)/(2.0 * true_pos + false_pos + false_neg)\n","    \n","    return coef_val\n","\n","def dice(y_true, y_pred, const=K.epsilon()):\n","    '''\n","    Sørensen–Dice Loss.\n","    \n","    dice(y_true, y_pred, const=K.epsilon())\n","    \n","    Input\n","    ----------\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    loss_val = 1 - dice_coef(y_true, y_pred, const=const)\n","    \n","    #BCE =  K.binary_crossentropy(y_true,y_pred)\n","    return loss_val # + BCE\n","\n","def tversky_coef(y_true, y_pred, alpha=0.7, const=K.epsilon()):\n","    '''\n","    Weighted Sørensen–Dice coefficient.\n","    \n","    Input\n","    ----------\n","        y_true, y_pred: predicted outputs and targets.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    \n","    # flatten 2-d tensors\n","    y_true_pos = tf.reshape(y_true, [-1])\n","    y_pred_pos = tf.reshape(y_pred, [-1])\n","    \n","    # get true pos (TP), false neg (FN), false pos (FP).\n","    true_pos  = tf.reduce_sum(y_true_pos * y_pred_pos)\n","    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n","    false_pos = tf.reduce_sum((1-y_true_pos) * y_pred_pos)\n","    \n","    # TP/(TP + a*FN + b*FP); a+b = 1\n","    coef_val = (true_pos + const)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + const)\n","    \n","    return coef_val\n","\n","def tversky(y_true, y_pred, alpha=0.7, const=K.epsilon()):\n","    '''\n","    Tversky Loss.\n","    \n","    tversky(y_true, y_pred, alpha=0.5, const=K.epsilon())\n","    \n","    ----------\n","    Hashemi, S.R., Salehi, S.S.M., Erdogmus, D., Prabhu, S.P., Warfield, S.K. and Gholipour, A., 2018. \n","    Tversky as a loss function for highly unbalanced image segmentation using 3d fully convolutional deep networks. \n","    arXiv preprint arXiv:1803.11078.\n","    \n","    Input\n","    ----------\n","        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    loss_val = 1 - tversky_coef(y_true, y_pred, alpha=alpha, const=const)\n","    \n","    return loss_val\n","\n","def focal_tversky(y_true, y_pred, alpha=0.7, gamma=4/3, const=K.epsilon()):\n","    \n","    '''\n","    Focal Tversky Loss (FTL)\n","    \n","    focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n","    \n","    ----------\n","    Abraham, N. and Khan, N.M., 2019, April. A novel focal tversky loss function with improved \n","    attention u-net for lesion segmentation. In 2019 IEEE 16th International Symposium on Biomedical Imaging \n","    (ISBI 2019) (pp. 683-687). IEEE.\n","    \n","    ----------\n","    Input\n","        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases \n","        gamma: tunable parameter within [1, 3].\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    # (Tversky loss)**(1/gamma) \n","    loss_val = tf.math.pow((1-tversky_coef(y_true, y_pred, alpha=alpha, const=const)), 1/gamma)\n","    \n","    return loss_val\n","\n","def suggest_filter_size(image1_batch,image2_batch,power_factors,filter_size):\n","      shape1= image1_batch.shape[1:-1]  \n","      shape2= image2_batch.shape[1:-1] \n","      if not(shape1[-3:-1][0]/(2**(len(power_factors)-1)) and shape2[-3:-1][0]/(2**(len(power_factors)-1)) >= filter_size):\n","            H = tf.math.reduce_min((shape1,shape2))\n","            suggested_filter_size = int(H/(2**(len(power_factors)-1)))\n","      else:\n","            suggested_filter_size = filter_size\n","      return suggested_filter_size\n","\n","def ms_ssim(y_true, y_pred):\n","    \"\"\"\n","    Multiscale structural similarity (MS-SSIM) loss.\n","    \n","    ms_ssim(y_true, y_pred, **tf_ssim_kw)\n","    \n","    ----------\n","    Wang, Z., Simoncelli, E.P. and Bovik, A.C., 2003, November. Multiscale structural similarity for image quality assessment. \n","    In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003 (Vol. 2, pp. 1398-1402). Ieee.\n","    \n","    ----------\n","    Input\n","        kwargs: keywords of `tf.image.ssim_multiscale`\n","                https://www.tensorflow.org/api_docs/python/tf/image/ssim_multiscale\n","                \n","        *Issues of `tf.image.ssim_multiscale`refers to:\n","                https://stackoverflow.com/questions/57127626/error-in-calculation-of-inbuilt-ms-ssim-function-in-tensorflow\n","    \n","    \"\"\"\n","    \n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    \n","    \n","    tf_ms_ssim = tf.image.ssim_multiscale(y_true, y_pred, max_val=1.0,filter_size=1)\n","        \n","    return 1 - tf_ms_ssim\n","\n","def DiceBCELoss2(targets, inputs, smooth=1e-6):    \n","       \n","    #flatten label and prediction tensors\n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","    \n","    BCE =  K.binary_crossentropy(targets, inputs)\n","    intersection = K.sum(inputs * targets)\n","    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    Dice_BCE = BCE + dice_loss\n","    \n","    return Dice_BCE\n","\n","\n","BACKBONE = 'resnet50'\n","LR = 0.0001\n","optim = tf.keras.optimizers.Adam(LR)\n","\n","\n","\n","model = sm.Unet(BACKBONE, encoder_weights='imagenet', classes=1, activation='sigmoid', input_shape=(512, 512, 3))\n","\n","model.compile(optimizer=optim, loss =  focal_tversky, metrics=['accuracy',sm.metrics.IOUScore(threshold=0.5)]) \n","\n","\n","\n","# callbacks=[early_stopping_monitor],\n","history = model.fit(x_train,y_train,batch_size=4,epochs=100,verbose=1)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:31:20.852349Z","iopub.status.busy":"2023-05-27T16:31:20.851201Z","iopub.status.idle":"2023-05-27T16:31:21.794920Z","shell.execute_reply":"2023-05-27T16:31:21.793660Z","shell.execute_reply.started":"2023-05-27T16:31:20.852307Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:29:33.189489Z","iopub.status.busy":"2023-05-27T16:29:33.188320Z","iopub.status.idle":"2023-05-27T16:29:36.966834Z","shell.execute_reply":"2023-05-27T16:29:36.965586Z","shell.execute_reply.started":"2023-05-27T16:29:33.189444Z"},"trusted":true},"outputs":[],"source":["predict_img=model.predict(x_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(history.history.keys())\n","print(history.history['f1-score'])\n","plt.plot(history.history['f1-score'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(history.history['loss'])\n","\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.models import load_model\n","\n","def DiceBCELoss(targets, inputs, smooth=1e-6):    \n","       \n","    #flatten label and prediction tensors\n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","    \n","    BCE =  K.binary_crossentropy(targets, inputs)\n","    intersection = K.sum(inputs * targets)\n","    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    Dice_BCE = BCE + dice_loss\n","    \n","    return Dice_BCE\n","\n","model= load_model(\"Croppedv3loveu.h5\", compile=False)\n","model.compile(optimizer ='adam', loss = DiceBCELoss, metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:30:05.096057Z","iopub.status.busy":"2023-05-27T16:30:05.095239Z","iopub.status.idle":"2023-05-27T16:31:20.849196Z","shell.execute_reply":"2023-05-27T16:31:20.848225Z","shell.execute_reply.started":"2023-05-27T16:30:05.096003Z"},"trusted":true},"outputs":[],"source":["from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score,jaccard_score, roc_auc_score\n","def dice(pred, true, k = 1):\n","    intersection = np.sum(pred[true==k]) * 2.0\n","    dice = intersection / (np.sum(pred) + np.sum(true))\n","    return dice\n","\n","\n","\n","\n","def plot_side_by_side(original_image, predicted_image, rad, label, pp, dice_score):\n","\n","    print('Plotting ' + label)\n","    a, (orig_ax, pred_ax, rad_ax) = plt.subplots(1, 3, figsize=(60,15))\n","    orig_ax.set_axis_off()\n","    orig_ax.imshow(original_image, aspect=\"auto\")\n","    orig_ax.set_title(\"Original\", fontsize = 160)\n","    pred_ax.set_axis_off()\n","    pred_ax.imshow(predicted_image, aspect=\"auto\")\n","    pred_ax.set_title(\"Predicted\", fontsize = 160)\n","    rad_ax.set_axis_off()\n","    rad_ax.imshow(rad, aspect=\"auto\")\n","    rad_ax.set_title(\"Radiography\", fontsize = 160)\n","    \n","    txt = \"Dice: {:.6f}\".format(dice_score)\n","    plt.text(0.95,0.5,txt, transform=a.transFigure, size=100)\n","    pp.savefig(a, pad_inches=2, bbox_inches='tight')\n"," \n","    \n","\n","  \n","    plt.show()\n","    \n","    \n","pp = PdfPages('unet102.pdf')\n","\n","\n","true_list_new=[]\n","pred_list_new=[]\n","\n","dices=[]\n","\n","for i in range(len(x_test)):\n","   \n","    \n","    \n","    predict_img[i] = np.where(predict_img[i] > 0.25, 1, 0)\n","    predict_img1=(predict_img[i]>0.001)*1\n","\n","    y_test1=(y_test[i]>0.001)*1\n","    dice_score = dice(predict_img1, y_test1, k = 1)\n","    print (\"Dice Similarity: {}\".format(dice_score))\n","    plot_side_by_side(y_test[i], predict_img[i], x_test[i], str(i), pp, dice_score)\n","    true_list_new.append(y_test1)\n","    pred_list_new.append(predict_img1)\n","    dices.append(dice_score)\n","    \n","   \n","    \n","    \n","    \n","\n","\n","true_list_new=np.array(true_list_new)\n","pred_list_new=np.array(pred_list_new)\n","\n","true_list_new=true_list_new.flatten()\n","pred_list_new=pred_list_new.flatten()\n","\n","print (\"Accuracy : \", accuracy_score(true_list_new,pred_list_new)*100) \n","print (\"Dice : \", dice(true_list_new, pred_list_new))\n","\n","print (\"IOU : \", jaccard_score(true_list_new, pred_list_new))\n","print (\"F1 : \",  f1_score(true_list_new, pred_list_new))\n","\n","print('Recall: %.3f' % recall_score(true_list_new, pred_list_new))\n","print('Precision: %.3f' % precision_score(true_list_new, pred_list_new))\n","print('AUC: %.3f' % roc_auc_score(true_list_new, pred_list_new))\n","\n","txt1 = \"Accuracy : \" + str(accuracy_score(true_list_new,pred_list_new)*100)\n","txt2 = \"Dice : \" + str(dice(true_list_new, pred_list_new))\n","txt3 = \"IOU : \"+ str(jaccard_score(true_list_new, pred_list_new))\n","txt4 = \"F1 : \" +  str(f1_score(true_list_new, pred_list_new))\n","txt5 = \"Recall: {:.3f}\".format(recall_score(true_list_new, pred_list_new))\n","txt6 = \"Precision: {:.3f}\".format(precision_score(true_list_new, pred_list_new))\n","txt7 = \"AUC: {:.3f}\".format(roc_auc_score(true_list_new, pred_list_new))\n","\n","fig = plt.figure(figsize=(110.69,80.27))\n","plt.text(0.05,0.95,txt1, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.85,txt2, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.75,txt3, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.65,txt4, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.55,txt5, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.45,txt6, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.35,txt7, transform=fig.transFigure, size=124)\n","pp.savefig()\n","pp.close()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T14:45:19.120519Z","iopub.status.busy":"2023-05-21T14:45:19.120009Z","iopub.status.idle":"2023-05-21T14:45:21.895682Z","shell.execute_reply":"2023-05-21T14:45:21.894463Z","shell.execute_reply.started":"2023-05-21T14:45:19.120473Z"},"trusted":true},"outputs":[],"source":["model.save(\"11deeplabfocal_tversky0486.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install imutils"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def crop_image_v2(img, t):\n","    mask = img!=0\n","    mask = mask.any(2)\n","    mask0,mask1 = mask.any(0),mask.any(1)\n","    colstart, colend = mask0.argmax(), len(mask0)-mask0[::-1].argmax()+1\n","    rowstart, rowend = mask1.argmax(), len(mask1)-mask1[::-1].argmax()+1\n","    return t[rowstart:rowend, colstart:colend]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.models import load_model\n","\n","def dice(y_true, y_pred, const=K.epsilon()):\n","    '''\n","    Sørensen–Dice Loss.\n","    \n","    dice(y_true, y_pred, const=K.epsilon())\n","    \n","    Input\n","    ----------\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    loss_val = 1 - dice_coef(y_true, y_pred, const=const)\n","    \n","    #BCE =  K.binary_crossentropy(y_true,y_pred)\n","    return loss_val # + BCE\n","\n","model= load_model(\"detection_teeth\", compile=False)\n","#model.compile(optimizer ='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","model.compile(optimizer ='adam', loss = dice, metrics = ['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","def crop_img(img, scale=1.0):\n","    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n","    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n","    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n","    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n","    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n","    return img_cropped\n","\n","img=X[22]\n","img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n","img_cropped = crop_img(img, 0.70)\n","\n","new_x=[]\n","for img in X:\n","    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n","    img_cropped = crop_img(img, 0.70)\n","    width = int(512)\n","    height = int(512)\n","    dim = (width, height)\n","\n","    # resize image\n","    img_cropped = cv2.resize(img_cropped, dim, interpolation = cv2.INTER_AREA)\n","    new_x.append(img_cropped)\n","\n","plt.imshow(new_x[1])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import imageio\n","import cv2\n","from matplotlib.backends.backend_pdf import PdfPages\n","def crop_image_v2(img, t):\n","    mask = img!=0\n","    \n","    mask = mask.any(2)\n","    mask0,mask1 = mask.any(0),mask.any(1)\n","    colstart, colend = mask0.argmax(), len(mask0)-mask0[::-1].argmax()+1\n","    rowstart, rowend = mask1.argmax(), len(mask1)-mask1[::-1].argmax()+1\n","    return t[rowstart:rowend, colstart:colend]\n","\n","\n","\n","\n","\n","for i in range(len(Y)):\n","    teeth=X[i]\n","    teeth2=Y[i]\n","    img=cv2.imread(files[i])\n","\n","\n","    new_img=crop_image_v2(img, teeth)\n","    new_img2=crop_image_v2(img, teeth2)\n","    resize_shape=(512,512)\n","    new_img = cv2.resize(new_img, (512,512))\n","    new_img2 = cv2.resize(new_img2, (512,512))\n","    \n","    imageio.imwrite(os.path.join('croppedX101',str(i) + '.jpg'),new_img)\n","    imageio.imwrite(os.path.join('croppedY101',str(i) + '.jpg'),new_img2)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(len(predict_img)):\n","    predicted=predict_img[i]\n","    imageio.imwrite(os.path.join('tempo3',str(i) + '.jpg'),predicted)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import imageio\n","#imageio.imwrite('outfile.jpg',predict_img[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["subdirec='tempo3/'\n","files = os.listdir(subdirec)\n","files.sort(key= lambda filename: int(filename.split('.')[0]))\n","files = [os.path.join(subdirec, f) for f in files]\n","print(files)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["teeth=x_test[1]\n","img=predict_img[1]\n","def crop_image_v2(img, t):\n","    mask = img!=0\n","    mask = mask.any(2)\n","    mask0,mask1 = mask.any(0),mask.any(1)\n","    colstart, colend = mask0.argmax(), len(mask0)-mask0[::-1].argmax()+1\n","    rowstart, rowend = mask1.argmax(), len(mask1)-mask1[::-1].argmax()+1\n","    return t[rowstart:rowend, colstart:colend]\n","\n","new_img=crop_image_v2(img, teeth)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["subdirec='/kaggle/input/croppedcavities/croppedX/'\n","files = os.listdir(subdirec)\n","files.sort(key= lambda filename: int(filename.split('.')[0]))\n","files = [os.path.join(subdirec, f) for f in files]\n","print(files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras import backend as K\n","def tversky_coef(y_true, y_pred, alpha=0.7, const=K.epsilon()):\n","    '''\n","    Weighted Sørensen–Dice coefficient.\n","\n","    Input\n","    ----------\n","        y_true, y_pred: predicted outputs and targets.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","\n","    '''\n","\n","    # flatten 2-d tensors\n","    y_true_pos = tf.reshape(y_true, [-1])\n","    y_pred_pos = tf.reshape(y_pred, [-1])\n","\n","    # get true pos (TP), false neg (FN), false pos (FP).\n","    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n","    false_neg = tf.reduce_sum(y_true_pos * (1 - y_pred_pos))\n","    false_pos = tf.reduce_sum((1 - y_true_pos) * y_pred_pos)\n","\n","    # TP/(TP + a*FN + b*FP); a+b = 1\n","    const = 1\n","    coef_val = (true_pos + const) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + const)\n","\n","    return coef_val\n","\n","\n","def tversky(y_true, y_pred, alpha=0.7, const=K.epsilon()):\n","    '''\n","    Tversky Loss.\n","\n","    tversky(y_true, y_pred, alpha=0.5, const=K.epsilon())\n","\n","    ----------\n","    Hashemi, S.R., Salehi, S.S.M., Erdogmus, D., Prabhu, S.P., Warfield, S.K. and Gholipour, A., 2018.\n","    Tversky as a loss function for highly unbalanced image segmentation using 3d fully convolutional deep networks.\n","    arXiv preprint arXiv:1803.11078.\n","\n","    Input\n","    ----------\n","        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases.\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","\n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","\n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","\n","    loss_val = 1 - tversky_coef(y_true, y_pred, alpha=alpha, const=const)\n","\n","    return loss_val\n","from keras.models import load_model\n","model1= load_model(\"/kaggle/input/uploads/11unet_tversky054.h5\", compile=False)\n","model1.compile(optimizer ='adam', loss = tversky, metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def focal_tversky(y_true, y_pred, alpha=0.7, gamma=4/3, const=K.epsilon()):\n","    \n","    '''\n","    Focal Tversky Loss (FTL)\n","    \n","    focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n","    \n","    ----------\n","    Abraham, N. and Khan, N.M., 2019, April. A novel focal tversky loss function with improved \n","    attention u-net for lesion segmentation. In 2019 IEEE 16th International Symposium on Biomedical Imaging \n","    (ISBI 2019) (pp. 683-687). IEEE.\n","    \n","    ----------\n","    Input\n","        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases \n","        gamma: tunable parameter within [1, 3].\n","        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n","        \n","    '''\n","    # tf tensor casting\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    \n","    # <--- squeeze-out length-1 dimensions.\n","    y_pred = tf.squeeze(y_pred)\n","    y_true = tf.squeeze(y_true)\n","    \n","    # (Tversky loss)**(1/gamma) \n","    loss_val = tf.math.pow((1-tversky_coef(y_true, y_pred, alpha=alpha, const=const)), 1/gamma)\n","    \n","    \n","    return loss_val\n","\n","model2= load_model(\"/kaggle/input/uploads/11fpnfocaltversky05091.h5\", compile=False)\n","model2.compile(optimizer ='adam', loss = focal_tversky, metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model3= load_model(\"/kaggle/input/uploads/11deeplabfocal_tversky0486.h5\", compile=False)\n","model3.compile(optimizer ='adam', loss = focal_tversky, metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predict_img1=model1.predict(x_test)\n","predict_img2=model2.predict(x_test)\n","predict_img3=model3.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score,jaccard_score, roc_auc_score\n","def dice(pred, true, k = 1):\n","    intersection = np.sum(pred[true==k]) * 2.0\n","    dice = intersection / (np.sum(pred) + np.sum(true))\n","    return dice\n","\n","\n","\n","\n","def plot_side_by_side(original_image, predicted_image1, predicted_image2, predicted_image3, predicted_image4, rad, label, pp, dice_score):\n","\n","    print('Plotting ' + label)\n","    a, (rad_ax, orig_ax, pred_ax1) = plt.subplots(1, 3, figsize=(60,15)) # figsize=(45,5)) #pred_ax2, pred_ax3, pred_ax4\n","    rad_ax.set_axis_off()\n","    rad_ax.imshow(rad, aspect=\"auto\")\n","    rad_ax.set_title(\"Radiography\", fontsize = 60)\n","    orig_ax.set_axis_off()\n","    orig_ax.imshow(original_image, aspect=\"auto\")\n","    orig_ax.set_title(\"Original\", fontsize = 60)\n","    pred_ax1.set_axis_off()\n","    pred_ax1.imshow(predicted_image4, aspect=\"auto\")\n","    pred_ax1.set_title(\"Combined\", fontsize = 60)\n","    \"\"\"\n","    pred_ax2.set_axis_off()\n","    pred_ax2.imshow(predicted_image1, aspect=\"auto\")\n","    pred_ax2.set_title(\"Unet\", fontsize = 60)\n","    pred_ax3.set_axis_off()\n","    pred_ax3.imshow(predicted_image2, aspect=\"auto\")\n","    pred_ax3.set_title(\"Fpn\", fontsize = 60)\n","    pred_ax4.set_axis_off()\n","    pred_ax4.imshow(predicted_image3, aspect=\"auto\")\n","    pred_ax4.set_title(\"Deeplab\", fontsize = 60)\n","    \"\"\"\n","    \n","    \n","    txt = \"Dice: {:.6f}\".format(dice_score)\n","    plt.text(0.95,0.5,txt, transform=a.transFigure, size=100)\n","    pp.savefig(a, pad_inches=2, bbox_inches='tight')\n"," \n","    \n","\n","  \n","    plt.show()\n","    \n","    \n","pp = PdfPages('11combined_dice068justcombined2.pdf')\n","\n","\n","true_list_new=[]\n","pred_list_new=[]\n","\n","dices=[]\n","predict_img4=[]\n","#predict_img=y_pred\n","#print(len(y_pred))\n","for i in range(len(x_test)):\n","   \n","    \n","    combined = predict_img1[i]  + predict_img2[i] + predict_img3[i]\n","    predict_img1[i] = np.where(predict_img1[i] > 0.15, 1, 0)\n","    predict_img2[i] = np.where(predict_img2[i] > 0.15, 1, 0)\n","    predict_img3[i] = np.where(predict_img3[i] > 0.45, 1, 0)\n","    combined = np.where(combined >= 1.05, 1, 0)\n","    \n","    predict_img11=(combined>0.001)*1\n","\n","    y_test1=(y_test[i]>0.001)*1\n","        #score=f1_score(y_test1[i].flatten(), predict_img1[i].flatten(), average='micro')\n","        #print(score)\n","    dice_score = dice(predict_img11, y_test1, k = 1) #255 in my case, can be 1 \n","    print (\"Dice Similarity: {}\".format(dice_score))\n","    plot_side_by_side(y_test[i], predict_img1[i], predict_img2[i], predict_img3[i], combined, x_test[i], str(i), pp, dice_score)\n","    true_list_new.append(y_test1)\n","    pred_list_new.append(predict_img11)\n","    dices.append(dice_score)\n","    \n","   \n","    \n","    \n","    \n","\n","\n","true_list_new=np.array(true_list_new)\n","pred_list_new=np.array(pred_list_new)\n","\n","true_list_new=true_list_new.flatten()\n","pred_list_new=pred_list_new.flatten()\n","\n","print (\"Accuracy : \", accuracy_score(true_list_new,pred_list_new)*100) \n","print (\"Dice : \", dice(true_list_new,pred_list_new)) \n","\n","print (\"IOU : \", jaccard_score(true_list_new, pred_list_new))\n","print (\"F1 : \",  f1_score(true_list_new, pred_list_new))\n","\n","print('Recall: %.3f' % recall_score(true_list_new, pred_list_new))\n","print('Precision: %.3f' % precision_score(true_list_new, pred_list_new))\n","print('AUC: %.3f' % roc_auc_score(true_list_new, pred_list_new))\n","\n","txt1 = \"Accuracy : \" + str(accuracy_score(true_list_new,pred_list_new)*100)\n","txt2 = \"Dice : \" + str(dice(true_list_new,pred_list_new))\n","txt3 = \"IOU : \"+ str(jaccard_score(true_list_new, pred_list_new))\n","txt4 = \"F1 : \" +  str(f1_score(true_list_new, pred_list_new))\n","txt5 = \"Recall: {:.3f}\".format(recall_score(true_list_new, pred_list_new))\n","txt6 = \"Precision: {:.3f}\".format(precision_score(true_list_new, pred_list_new))\n","txt7 = \"AUC: {:.3f}\".format(roc_auc_score(true_list_new, pred_list_new))\n","\n","fig = plt.figure(figsize=(110.69,80.27))\n","plt.text(0.05,0.95,txt1, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.85,txt2, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.75,txt3, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.65,txt4, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.55,txt5, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.45,txt6, transform=fig.transFigure, size=124)\n","plt.text(0.05,0.35,txt7, transform=fig.transFigure, size=124)\n","pp.savefig()\n","pp.close()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
